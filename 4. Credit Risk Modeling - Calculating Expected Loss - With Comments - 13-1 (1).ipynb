{"cells":[{"cell_type":"markdown","metadata":{"id":"Puj5Qe9rDnrj"},"source":["# Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JAJ-0H2IDnrl"},"outputs":[],"source":["import numpy as np\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"DIRsw47jDnrn"},"source":["# Import Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b41Sl9QnDnrn"},"outputs":[],"source":["# Import data.\n","loan_data_preprocessed_backup = pd.read_csv('loan_data_2007_2014_preprocessed.csv')"]},{"cell_type":"markdown","metadata":{"id":"8726kdxUDnro"},"source":["# Explore Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cSRoEn7MDnro"},"outputs":[],"source":["loan_data_preprocessed = loan_data_preprocessed_backup.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3piQEx-wDnrp"},"outputs":[],"source":["loan_data_preprocessed.columns.values\n","# Displays all column names."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"_7bHl1yrDnrq"},"outputs":[],"source":["loan_data_preprocessed.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Mo8Y_htDnrr"},"outputs":[],"source":["loan_data_preprocessed.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"njgilRtUDnrr"},"outputs":[],"source":["loan_data_defaults = loan_data_preprocessed[loan_data_preprocessed['loan_status'].isin(['Charged Off','Does not meet the credit policy. Status:Charged Off'])]\n","# Here we take only the accounts that were charged-off (written-off)."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"qYuH-vkfDnrs"},"outputs":[],"source":["loan_data_defaults.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oVO0hqGDDnrs"},"outputs":[],"source":["pd.options.display.max_rows = None\n","# Sets the pandas dataframe options to display all columns/ rows."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LywNlTroDnrs"},"outputs":[],"source":["loan_data_defaults.isnull().sum()"]},{"cell_type":"markdown","metadata":{"id":"TdVLlPEoDnrt"},"source":["# Independent Variables"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"AmsVHzgODnrt"},"outputs":[],"source":["loan_data_defaults['mths_since_last_delinq'].fillna(0, inplace = True)\n","# We fill the missing values with zeroes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Koc16s2UDnrt"},"outputs":[],"source":["#loan_data_defaults['mths_since_last_delinq'].fillna(loan_data_defaults['mths_since_last_delinq'].max() + 12, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vZDaiwJGDnru"},"outputs":[],"source":["loan_data_defaults['mths_since_last_record'].fillna(0, inplace=True)\n","# We fill the missing values with zeroes."]},{"cell_type":"markdown","metadata":{"id":"j3brAsaEDnru"},"source":["# Dependent Variables"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"dxAzTwzyDnru"},"outputs":[],"source":["loan_data_defaults['recovery_rate'] = loan_data_defaults['recoveries'] / loan_data_defaults['funded_amnt']\n","# We calculate the dependent variable for the LGD model: recovery rate.\n","# It is the ratio of recoveries and funded amount."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hx4DbVBXDnru"},"outputs":[],"source":["loan_data_defaults['recovery_rate'].describe()\n","# Shows some descriptive statisics for the values of a column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8qU1px6Dnru"},"outputs":[],"source":["loan_data_defaults['recovery_rate'] = np.where(loan_data_defaults['recovery_rate'] > 1, 1, loan_data_defaults['recovery_rate'])\n","loan_data_defaults['recovery_rate'] = np.where(loan_data_defaults['recovery_rate'] < 0, 0, loan_data_defaults['recovery_rate'])\n","# We set recovery rates that are greater than 1 to 1 and recovery rates that are less than 0 to 0."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-tRLHbjtDnrv"},"outputs":[],"source":["loan_data_defaults['recovery_rate'].describe()\n","# Shows some descriptive statisics for the values of a column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u722bU-GDnrv"},"outputs":[],"source":["loan_data_defaults['CCF'] = (loan_data_defaults['funded_amnt'] - loan_data_defaults['total_rec_prncp']) / loan_data_defaults['funded_amnt']\n","# We calculate the dependent variable for the EAD model: credit conversion factor.\n","# It is the ratio of the difference of the amount used at the moment of default to the total funded amount."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XNjW8SscDnrv"},"outputs":[],"source":["loan_data_defaults['CCF'].describe()\n","# Shows some descriptive statisics for the values of a column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pD95oAnxDnrv"},"outputs":[],"source":["loan_data_defaults.to_csv('loan_data_defaults.csv')\n","# We save the data to a CSV file."]},{"cell_type":"markdown","metadata":{"id":"u2pTmVoHDnrv"},"source":["# Explore Dependent Variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UzDogahvDnrw"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zoFLgHntDnrw"},"outputs":[],"source":["plt.hist(loan_data_defaults['recovery_rate'], bins = 100)\n","# We plot a histogram of a variable with 100 bins."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AWz7gjRjDnrw"},"outputs":[],"source":["plt.hist(loan_data_defaults['recovery_rate'], bins = 50)\n","# We plot a histogram of a variable with 50 bins."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dS5rMvqZDnrw"},"outputs":[],"source":["plt.hist(loan_data_defaults['CCF'], bins = 100)\n","# We plot a histogram of a variable with 100 bins."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uhns8HQIDnrw"},"outputs":[],"source":["loan_data_defaults['recovery_rate_0_1'] = np.where(loan_data_defaults['recovery_rate'] == 0, 0, 1)\n","# We create a new variable which is 0 if recovery rate is 0 and 1 otherwise."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"290CQ6ohDnrx"},"outputs":[],"source":["loan_data_defaults['recovery_rate_0_1']"]},{"cell_type":"markdown","metadata":{"id":"Y6D3XfhEDnrx"},"source":["# LGD Model"]},{"cell_type":"markdown","metadata":{"id":"jCEAPXg6Dnrx"},"source":["### Splitting Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9dxaIYA5Dnrx"},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KWlrU1coDnrx"},"outputs":[],"source":["# LGD model stage 1 datasets: recovery rate 0 or greater than 0.\n","lgd_inputs_stage_1_train, lgd_inputs_stage_1_test, lgd_targets_stage_1_train, lgd_targets_stage_1_test = train_test_split(loan_data_defaults.drop(['good_bad', 'recovery_rate','recovery_rate_0_1', 'CCF'], axis = 1), loan_data_defaults['recovery_rate_0_1'], test_size = 0.2, random_state = 42)\n","# Takes a set of inputs and a set of targets as arguments. Splits the inputs and the targets into four dataframes:\n","# Inputs - Train, Inputs - Test, Targets - Train, Targets - Test."]},{"cell_type":"markdown","metadata":{"id":"PmsUe7pgDnry"},"source":["### Preparing the Inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cs6WRHCGDnry"},"outputs":[],"source":["features_all = ['grade:A',\n","'grade:B',\n","'grade:C',\n","'grade:D',\n","'grade:E',\n","'grade:F',\n","'grade:G',\n","'home_ownership:MORTGAGE',\n","'home_ownership:NONE',\n","'home_ownership:OTHER',\n","'home_ownership:OWN',\n","'home_ownership:RENT',\n","'verification_status:Not Verified',\n","'verification_status:Source Verified',\n","'verification_status:Verified',\n","'purpose:car',\n","'purpose:credit_card',\n","'purpose:debt_consolidation',\n","'purpose:educational',\n","'purpose:home_improvement',\n","'purpose:house',\n","'purpose:major_purchase',\n","'purpose:medical',\n","'purpose:moving',\n","'purpose:other',\n","'purpose:renewable_energy',\n","'purpose:small_business',\n","'purpose:vacation',\n","'purpose:wedding',\n","'initial_list_status:f',\n","'initial_list_status:w',\n","'term_int',\n","'emp_length_int',\n","'mths_since_issue_d',\n","'mths_since_earliest_cr_line',\n","'funded_amnt',\n","'int_rate',\n","'installment',\n","'annual_inc',\n","'dti',\n","'delinq_2yrs',\n","'inq_last_6mths',\n","'mths_since_last_delinq',\n","'mths_since_last_record',\n","'open_acc',\n","'pub_rec',\n","'total_acc',\n","'acc_now_delinq',\n","'total_rev_hi_lim']\n","# List of all independent variables for the models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b5SxZi9SDnry"},"outputs":[],"source":["features_reference_cat = ['grade:G',\n","'home_ownership:RENT',\n","'verification_status:Verified',\n","'purpose:credit_card',\n","'initial_list_status:f']\n","# List of the dummy variable reference categories."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VEvMaZ7gDnr6"},"outputs":[],"source":["lgd_inputs_stage_1_train = lgd_inputs_stage_1_train[features_all]\n","# Here we keep only the variables we need for the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rk25HGXGDnr7"},"outputs":[],"source":["lgd_inputs_stage_1_train = lgd_inputs_stage_1_train.drop(features_reference_cat, axis = 1)\n","# Here we remove the dummy variable reference categories."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XYOEJf3mDnr7"},"outputs":[],"source":["lgd_inputs_stage_1_train.isnull().sum()\n","# Check for missing values. We check whether the value of each row for each column is missing or not,\n","# then sum accross columns."]},{"cell_type":"markdown","metadata":{"id":"Jy2IxBDVDnr7"},"source":["### Estimating the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tU0tKix4Dnr7"},"outputs":[],"source":["# P values for sklearn logistic regression.\n","\n","# Class to display p-values for logistic regression in sklearn.\n","\n","from sklearn import linear_model\n","import scipy.stats as stat\n","\n","class LogisticRegression_with_p_values:\n","\n","    def __init__(self,*args,**kwargs):#,**kwargs):\n","        self.model = linear_model.LogisticRegression(*args,**kwargs)#,**args)\n","\n","    def fit(self,X,y):\n","        self.model.fit(X,y)\n","\n","        #### Get p-values for the fitted model ####\n","        denom = (2.0 * (1.0 + np.cosh(self.model.decision_function(X))))\n","        denom = np.tile(denom,(X.shape[1],1)).T\n","        F_ij = np.dot((X / denom).T,X) ## Fisher Information Matrix\n","        Cramer_Rao = np.linalg.inv(F_ij) ## Inverse Information Matrix\n","        sigma_estimates = np.sqrt(np.diagonal(Cramer_Rao))\n","        z_scores = self.model.coef_[0] / sigma_estimates # z-score for eaach model coefficient\n","        p_values = [stat.norm.sf(abs(x)) * 2 for x in z_scores] ### two tailed test for p-values\n","\n","        self.coef_ = self.model.coef_\n","        self.intercept_ = self.model.intercept_\n","        #self.z_scores = z_scores\n","        self.p_values = p_values\n","        #self.sigma_estimates = sigma_estimates\n","        #self.F_ij = F_ij"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dAUcdrBhDnr8"},"outputs":[],"source":["reg_lgd_st_1 = LogisticRegression_with_p_values()\n","# We create an instance of an object from the 'LogisticRegression' class.\n","reg_lgd_st_1.fit(lgd_inputs_stage_1_train, lgd_targets_stage_1_train)\n","# Estimates the coefficients of the object from the 'LogisticRegression' class\n","# with inputs (independent variables) contained in the first dataframe\n","# and targets (dependent variables) contained in the second dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_IJU0IbzDnr8"},"outputs":[],"source":["feature_name = lgd_inputs_stage_1_train.columns.values\n","# Stores the names of the columns of a dataframe in a variable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2P0IKY7dDnr8"},"outputs":[],"source":["summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n","# Creates a dataframe with a column titled 'Feature name' and row values contained in the 'feature_name' variable.\n","summary_table['Coefficients'] = np.transpose(reg_lgd_st_1.coef_)\n","# Creates a new column in the dataframe, called 'Coefficients',\n","# with row values the transposed coefficients from the 'LogisticRegression' object.\n","summary_table.index = summary_table.index + 1\n","# Increases the index of every row of the dataframe with 1.\n","summary_table.loc[0] = ['Intercept', reg_lgd_st_1.intercept_[0]]\n","# Assigns values of the row with index 0 of the dataframe.\n","summary_table = summary_table.sort_index()\n","# Sorts the dataframe by index.\n","p_values = reg_lgd_st_1.p_values\n","# We take the result of the newly added method 'p_values' and store it in a variable 'p_values'.\n","p_values = np.append(np.nan,np.array(p_values))\n","# We add the value 'NaN' in the beginning of the variable with p-values.\n","summary_table['p_values'] = p_values\n","# In the 'summary_table' dataframe, we add a new column, called 'p_values', containing the values from the 'p_values' variable.\n","summary_table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f8etMWTwDnr9"},"outputs":[],"source":["summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n","summary_table['Coefficients'] = np.transpose(reg_lgd_st_1.coef_)\n","summary_table.index = summary_table.index + 1\n","summary_table.loc[0] = ['Intercept', reg_lgd_st_1.intercept_[0]]\n","summary_table = summary_table.sort_index()\n","p_values = reg_lgd_st_1.p_values\n","p_values = np.append(np.nan,np.array(p_values))\n","summary_table['p_values'] = p_values\n","summary_table"]},{"cell_type":"markdown","metadata":{"id":"_qtoxswNDnr9"},"source":["### Testing the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v82e9cR-Dnr9"},"outputs":[],"source":["lgd_inputs_stage_1_test = lgd_inputs_stage_1_test[features_all]\n","# Here we keep only the variables we need for the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZX8A4Eo4Dnr9"},"outputs":[],"source":["lgd_inputs_stage_1_test = lgd_inputs_stage_1_test.drop(features_reference_cat, axis = 1)\n","# Here we remove the dummy variable reference categories."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0HD1hLL7Dnr9"},"outputs":[],"source":["y_hat_test_lgd_stage_1 = reg_lgd_st_1.model.predict(lgd_inputs_stage_1_test)\n","# Calculates the predicted values for the dependent variable (targets)\n","# based on the values of the independent variables (inputs) supplied as an argument."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e3atHqekDnr-"},"outputs":[],"source":["y_hat_test_lgd_stage_1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"guX1nyByDnr-"},"outputs":[],"source":["y_hat_test_proba_lgd_stage_1 = reg_lgd_st_1.model.predict_proba(lgd_inputs_stage_1_test)\n","# Calculates the predicted probability values for the dependent variable (targets)\n","# based on the values of the independent variables (inputs) supplied as an argument."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9YSwFQagDnr-"},"outputs":[],"source":["y_hat_test_proba_lgd_stage_1\n","# This is an array of arrays of predicted class probabilities for all classes.\n","# In this case, the first value of every sub-array is the probability for the observation to belong to the first class, i.e. 0,\n","# and the second value is the probability for the observation to belong to the first class, i.e. 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J5x9g1Q0Dnr-"},"outputs":[],"source":["y_hat_test_proba_lgd_stage_1 = y_hat_test_proba_lgd_stage_1[: ][: , 1]\n","# Here we take all the arrays in the array, and from each array, we take all rows, and only the element with index 1,\n","# that is, the second element.\n","# In other words, we take only the probabilities for being 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"405QJKo6Dnr_"},"outputs":[],"source":["y_hat_test_proba_lgd_stage_1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dJu5TW4iDnr_"},"outputs":[],"source":["lgd_targets_stage_1_test_temp = lgd_targets_stage_1_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"628rkdh7Dnr_"},"outputs":[],"source":["lgd_targets_stage_1_test_temp.reset_index(drop = True, inplace = True)\n","# We reset the index of a dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9hY5L1RDDnr_"},"outputs":[],"source":["df_actual_predicted_probs = pd.concat([lgd_targets_stage_1_test_temp, pd.DataFrame(y_hat_test_proba_lgd_stage_1)], axis = 1)\n","# Concatenates two dataframes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J04fqF1TDnsA"},"outputs":[],"source":["df_actual_predicted_probs.columns = ['lgd_targets_stage_1_test', 'y_hat_test_proba_lgd_stage_1']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nXV3xx4fDnsA"},"outputs":[],"source":["df_actual_predicted_probs.index = lgd_inputs_stage_1_test.index\n","# Makes the index of one dataframe equal to the index of another dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T2sSxTXPDnsA"},"outputs":[],"source":["df_actual_predicted_probs.head()"]},{"cell_type":"markdown","metadata":{"id":"OPxjcRsnDnsA"},"source":["### Estimating the Аccuracy of the Мodel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"soi7ovlrDnsB"},"outputs":[],"source":["tr = 0.5\n","# We create a new column with an indicator,\n","# where every observation that has predicted probability greater than the threshold has a value of 1,\n","# and every observation that has predicted probability lower than the threshold has a value of 0.\n","df_actual_predicted_probs['y_hat_test_lgd_stage_1'] = np.where(df_actual_predicted_probs['y_hat_test_proba_lgd_stage_1'] > tr, 1, 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"txOPOKJJDnsB"},"outputs":[],"source":["pd.crosstab(df_actual_predicted_probs['lgd_targets_stage_1_test'], df_actual_predicted_probs['y_hat_test_lgd_stage_1'], rownames = ['Actual'], colnames = ['Predicted'])\n","# Creates a cross-table where the actual values are displayed by rows and the predicted values by columns.\n","# This table is known as a Confusion Matrix."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aUPo8GyZDnsB"},"outputs":[],"source":["pd.crosstab(df_actual_predicted_probs['lgd_targets_stage_1_test'], df_actual_predicted_probs['y_hat_test_lgd_stage_1'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]\n","# Here we divide each value of the table by the total number of observations,\n","# thus getting percentages, or, rates."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aTbopg4ZDnsB"},"outputs":[],"source":["(pd.crosstab(df_actual_predicted_probs['lgd_targets_stage_1_test'], df_actual_predicted_probs['y_hat_test_lgd_stage_1'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[0, 0] + (pd.crosstab(df_actual_predicted_probs['lgd_targets_stage_1_test'], df_actual_predicted_probs['y_hat_test_lgd_stage_1'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[1, 1]\n","# Here we calculate Accuracy of the model, which is the sum of the diagonal rates."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KSQQmSg_DnsC"},"outputs":[],"source":["from sklearn.metrics import roc_curve, roc_auc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9I7OFJPDDnsC"},"outputs":[],"source":["fpr, tpr, thresholds = roc_curve(df_actual_predicted_probs['lgd_targets_stage_1_test'], df_actual_predicted_probs['y_hat_test_proba_lgd_stage_1'])\n","# Returns the Receiver Operating Characteristic (ROC) Curve from a set of actual values and their predicted probabilities.\n","# As a result, we get three arrays: the false positive rates, the true positive rates, and the thresholds.\n","# we store each of the three arrays in a separate variable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bZKlEc84DnsC"},"outputs":[],"source":["plt.plot(fpr, tpr)\n","# We plot the false positive rate along the x-axis and the true positive rate along the y-axis,\n","# thus plotting the ROC curve.\n","plt.plot(fpr, fpr, linestyle = '--', color = 'k')\n","# We plot a seconary diagonal line, with dashed line style and black color.\n","plt.xlabel('False positive rate')\n","# We name the x-axis \"False positive rate\".\n","plt.ylabel('True positive rate')\n","# We name the x-axis \"True positive rate\".\n","plt.title('ROC curve')\n","# We name the graph \"ROC curve\"."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MnhTibmBDnsC"},"outputs":[],"source":["AUROC = roc_auc_score(df_actual_predicted_probs['lgd_targets_stage_1_test'], df_actual_predicted_probs['y_hat_test_proba_lgd_stage_1'])\n","# Calculates the Area Under the Receiver Operating Characteristic Curve (AUROC)\n","# from a set of actual values and their predicted probabilities.\n","AUROC"]},{"cell_type":"markdown","metadata":{"id":"vuo7PheeDnsD"},"source":["### Saving the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rm6WdWpLDnsD"},"outputs":[],"source":["import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RGRQpehpDnsD"},"outputs":[],"source":["pickle.dump(reg_lgd_st_1, open('lgd_model_stage_1.sav', 'wb'))\n","# Here we export our model to a 'SAV' file with file name 'lgd_model_stage_1.sav'."]},{"cell_type":"markdown","metadata":{"id":"VPgq6zVIDnsD"},"source":["### Stage 2 – Linear Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GDbKEr4bDnsD"},"outputs":[],"source":["lgd_stage_2_data = loan_data_defaults[loan_data_defaults['recovery_rate_0_1'] == 1]\n","# Here we take only rows where the original recovery rate variable is greater than one,\n","# i.e. where the indicator variable we created is equal to 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k37mJF4gDnsE"},"outputs":[],"source":["# LGD model stage 2 datasets: how much more than 0 is the recovery rate\n","lgd_inputs_stage_2_train, lgd_inputs_stage_2_test, lgd_targets_stage_2_train, lgd_targets_stage_2_test = train_test_split(lgd_stage_2_data.drop(['good_bad', 'recovery_rate','recovery_rate_0_1', 'CCF'], axis = 1), lgd_stage_2_data['recovery_rate'], test_size = 0.2, random_state = 42)\n","# Takes a set of inputs and a set of targets as arguments. Splits the inputs and the targets into four dataframes:\n","# Inputs - Train, Inputs - Test, Targets - Train, Targets - Test."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"APzPSUALDnsE"},"outputs":[],"source":["from sklearn import linear_model\n","from sklearn.metrics import mean_squared_error, r2_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UhqTp2pkDnsE"},"outputs":[],"source":["# Since the p-values are obtained through certain statistics, we need the 'stat' module from scipy.stats\n","import scipy.stats as stat\n","\n","# Since we are using an object oriented language such as Python, we can simply define our own\n","# LinearRegression class (the same one from sklearn)\n","# By typing the code below we will ovewrite a part of the class with one that includes p-values\n","# Here's the full source code of the ORIGINAL class: https://github.com/scikit-learn/scikit-learn/blob/7b136e9/sklearn/linear_model/base.py#L362\n","\n","\n","class LinearRegression(linear_model.LinearRegression):\n","    \"\"\"\n","    LinearRegression class after sklearn's, but calculate t-statistics\n","    and p-values for model coefficients (betas).\n","    Additional attributes available after .fit()\n","    are `t` and `p` which are of the shape (y.shape[1], X.shape[1])\n","    which is (n_features, n_coefs)\n","    This class sets the intercept to 0 by default, since usually we include it\n","    in X.\n","    \"\"\"\n","\n","    # nothing changes in __init__\n","    def __init__(self, fit_intercept=True, normalize=False, copy_X=True,\n","                 n_jobs=1):\n","        self.fit_intercept = fit_intercept\n","        self.normalize = normalize\n","        self.copy_X = copy_X\n","        self.n_jobs = n_jobs\n","\n","\n","    def fit(self, X, y, n_jobs=1):\n","        self = super(LinearRegression, self).fit(X, y, n_jobs)\n","\n","        # Calculate SSE (sum of squared errors)\n","        # and SE (standard error)\n","        sse = np.sum((self.predict(X) - y) ** 2, axis=0) / float(X.shape[0] - X.shape[1])\n","        se = np.array([np.sqrt(np.diagonal(sse * np.linalg.inv(np.dot(X.T, X))))])\n","\n","        # compute the t-statistic for each feature\n","        self.t = self.coef_ / se\n","        # find the p-value for each feature\n","        self.p = np.squeeze(2 * (1 - stat.t.cdf(np.abs(self.t), y.shape[0] - X.shape[1])))\n","        return self"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6UKrnYYNDnsE"},"outputs":[],"source":["import scipy.stats as stat\n","\n","class LinearRegression(linear_model.LinearRegression):\n","    def __init__(self, fit_intercept=True, normalize=False, copy_X=True,\n","                 n_jobs=1):\n","        self.fit_intercept = fit_intercept\n","        self.normalize = normalize\n","        self.copy_X = copy_X\n","        self.n_jobs = n_jobs\n","    def fit(self, X, y, n_jobs=1):\n","        self = super(LinearRegression, self).fit(X, y, n_jobs)\n","        sse = np.sum((self.predict(X) - y) ** 2, axis=0) / float(X.shape[0] - X.shape[1])\n","        se = np.array([np.sqrt(np.diagonal(sse * np.linalg.inv(np.dot(X.T, X))))])\n","        self.t = self.coef_ / se\n","        self.p = np.squeeze(2 * (1 - stat.t.cdf(np.abs(self.t), y.shape[0] - X.shape[1])))\n","        return self"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRAv2JgaDnsF"},"outputs":[],"source":["lgd_inputs_stage_2_train = lgd_inputs_stage_2_train[features_all]\n","# Here we keep only the variables we need for the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IfjYf2kBDnsF"},"outputs":[],"source":["lgd_inputs_stage_2_train = lgd_inputs_stage_2_train.drop(features_reference_cat, axis = 1)\n","# Here we remove the dummy variable reference categories."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eP9gTBIODnsF"},"outputs":[],"source":["reg_lgd_st_2 = LinearRegression()\n","# We create an instance of an object from the 'LogisticRegression' class.\n","reg_lgd_st_2.fit(lgd_inputs_stage_2_train, lgd_targets_stage_2_train)\n","# Estimates the coefficients of the object from the 'LogisticRegression' class\n","# with inputs (independent variables) contained in the first dataframe\n","# and targets (dependent variables) contained in the second dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iy023UKLDnsF"},"outputs":[],"source":["feature_name = lgd_inputs_stage_2_train.columns.values\n","# Stores the names of the columns of a dataframe in a variable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i-YAYC66DnsF"},"outputs":[],"source":["summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n","# Creates a dataframe with a column titled 'Feature name' and row values contained in the 'feature_name' variable.\n","summary_table['Coefficients'] = np.transpose(reg_lgd_st_2.coef_)\n","# Creates a new column in the dataframe, called 'Coefficients',\n","# with row values the transposed coefficients from the 'LogisticRegression' object.\n","summary_table.index = summary_table.index + 1\n","# Increases the index of every row of the dataframe with 1.\n","summary_table.loc[0] = ['Intercept', reg_lgd_st_2.intercept_]\n","# Assigns values of the row with index 0 of the dataframe.\n","summary_table = summary_table.sort_index()\n","# Sorts the dataframe by index.\n","p_values = reg_lgd_st_2.p\n","# We take the result of the newly added method 'p_values' and store it in a variable 'p_values'.\n","p_values = np.append(np.nan,np.array(p_values))\n","# We add the value 'NaN' in the beginning of the variable with p-values.\n","summary_table['p_values'] = p_values.round(3)\n","# In the 'summary_table' dataframe, we add a new column, called 'p_values', containing the values from the 'p_values' variable.\n","summary_table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SdAJ7KAzDnsG"},"outputs":[],"source":["summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n","summary_table['Coefficients'] = np.transpose(reg_lgd_st_2.coef_)\n","summary_table.index = summary_table.index + 1\n","summary_table.loc[0] = ['Intercept', reg_lgd_st_2.intercept_]\n","summary_table = summary_table.sort_index()\n","p_values = reg_lgd_st_2.p\n","p_values = np.append(np.nan,np.array(p_values))\n","summary_table['p_values'] = p_values.round(3)\n","summary_table"]},{"cell_type":"markdown","metadata":{"id":"usG5X0eNDnsG"},"source":["### Stage 2 – Linear Regression Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h60uJJUIDnsG"},"outputs":[],"source":["lgd_inputs_stage_2_test = lgd_inputs_stage_2_test[features_all]\n","# Here we keep only the variables we need for the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5OTOys5SDnsG"},"outputs":[],"source":["lgd_inputs_stage_2_test = lgd_inputs_stage_2_test.drop(features_reference_cat, axis = 1)\n","# Here we remove the dummy variable reference categories."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ULyDrqXsDnsH"},"outputs":[],"source":["lgd_inputs_stage_2_test.columns.values\n","# Calculates the predicted values for the dependent variable (targets)\n","# based on the values of the independent variables (inputs) supplied as an argument."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JgBT8VqvDnsH"},"outputs":[],"source":["y_hat_test_lgd_stage_2 = reg_lgd_st_2.predict(lgd_inputs_stage_2_test)\n","# Calculates the predicted values for the dependent variable (targets)\n","# based on the values of the independent variables (inputs) supplied as an argument."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h7Ajz_j7DnsH"},"outputs":[],"source":["lgd_targets_stage_2_test_temp = lgd_targets_stage_2_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UhBrjXU8DnsH"},"outputs":[],"source":["lgd_targets_stage_2_test_temp = lgd_targets_stage_2_test_temp.reset_index(drop = True)\n","# We reset the index of a dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uqYknfACDnsH"},"outputs":[],"source":["pd.concat([lgd_targets_stage_2_test_temp, pd.DataFrame(y_hat_test_lgd_stage_2)], axis = 1).corr()\n","# We calculate the correlation between actual and predicted values."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"tXDUCB9PDnsI"},"outputs":[],"source":["sns.distplot(lgd_targets_stage_2_test - y_hat_test_lgd_stage_2)\n","# We plot the distribution of the residuals."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yuAnT2fvDnsI"},"outputs":[],"source":["pickle.dump(reg_lgd_st_2, open('lgd_model_stage_2.sav', 'wb'))\n","# Here we export our model to a 'SAV' file with file name 'lgd_model_stage_1.sav'."]},{"cell_type":"markdown","metadata":{"id":"aFxMbV5mDnsI"},"source":["### Combining Stage 1 and Stage 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_34xV0rhDnsI"},"outputs":[],"source":["y_hat_test_lgd_stage_2_all = reg_lgd_st_2.predict(lgd_inputs_stage_1_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_D8JJWtPDnsI"},"outputs":[],"source":["y_hat_test_lgd_stage_2_all"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eyIsu8R9DnsJ"},"outputs":[],"source":["y_hat_test_lgd = y_hat_test_lgd_stage_1 * y_hat_test_lgd_stage_2_all\n","# Here we combine the predictions of the models from the two stages."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sYB5BiNSDnsJ"},"outputs":[],"source":["pd.DataFrame(y_hat_test_lgd).describe()\n","# Shows some descriptive statisics for the values of a column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h9doVlg9DnsJ"},"outputs":[],"source":["y_hat_test_lgd = np.where(y_hat_test_lgd < 0, 0, y_hat_test_lgd)\n","y_hat_test_lgd = np.where(y_hat_test_lgd > 1, 1, y_hat_test_lgd)\n","# We set predicted values that are greater than 1 to 1 and predicted values that are less than 0 to 0."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rA7hW6CxDnsJ"},"outputs":[],"source":["pd.DataFrame(y_hat_test_lgd).describe()\n","# Shows some descriptive statisics for the values of a column."]},{"cell_type":"markdown","metadata":{"id":"eBlUcG26DnsK"},"source":["# EAD Model"]},{"cell_type":"markdown","metadata":{"id":"qJl416bBDnsK"},"source":["### Estimation and Interpretation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wSrJbBbtDnsK"},"outputs":[],"source":["# EAD model datasets\n","ead_inputs_train, ead_inputs_test, ead_targets_train, ead_targets_test = train_test_split(loan_data_defaults.drop(['good_bad', 'recovery_rate','recovery_rate_0_1', 'CCF'], axis = 1), loan_data_defaults['CCF'], test_size = 0.2, random_state = 42)\n","# Takes a set of inputs and a set of targets as arguments. Splits the inputs and the targets into four dataframes:\n","# Inputs - Train, Inputs - Test, Targets - Train, Targets - Test."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oTKZ9LfLDnsK"},"outputs":[],"source":["ead_inputs_train.columns.values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mZLyKZJLDnsK"},"outputs":[],"source":["ead_inputs_train = ead_inputs_train[features_all]\n","# Here we keep only the variables we need for the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8E74RllhDnsL"},"outputs":[],"source":["ead_inputs_train = ead_inputs_train.drop(features_reference_cat, axis = 1)\n","# Here we remove the dummy variable reference categories."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TxpjzQIXDnsL"},"outputs":[],"source":["reg_ead = LinearRegression()\n","# We create an instance of an object from the 'LogisticRegression' class.\n","reg_ead.fit(ead_inputs_train, ead_targets_train)\n","# Estimates the coefficients of the object from the 'LogisticRegression' class\n","# with inputs (independent variables) contained in the first dataframe\n","# and targets (dependent variables) contained in the second dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JI3Ukz19DnsL"},"outputs":[],"source":["feature_name = ead_inputs_train.columns.values"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"lx_EeoQ4DnsL"},"outputs":[],"source":["summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n","# Creates a dataframe with a column titled 'Feature name' and row values contained in the 'feature_name' variable.\n","summary_table['Coefficients'] = np.transpose(reg_ead.coef_)\n","# Creates a new column in the dataframe, called 'Coefficients',\n","# with row values the transposed coefficients from the 'LogisticRegression' object.\n","summary_table.index = summary_table.index + 1\n","# Increases the index of every row of the dataframe with 1.\n","summary_table.loc[0] = ['Intercept', reg_ead.intercept_]\n","# Assigns values of the row with index 0 of the dataframe.\n","summary_table = summary_table.sort_index()\n","# Sorts the dataframe by index.\n","p_values = reg_lgd_st_2.p\n","# We take the result of the newly added method 'p_values' and store it in a variable 'p_values'.\n","p_values = np.append(np.nan,np.array(p_values))\n","# We add the value 'NaN' in the beginning of the variable with p-values.\n","summary_table['p_values'] = p_values\n","# In the 'summary_table' dataframe, we add a new column, called 'p_values', containing the values from the 'p_values' variable.\n","summary_table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1tFB_EnQDnsM"},"outputs":[],"source":["summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n","summary_table['Coefficients'] = np.transpose(reg_ead.coef_)\n","summary_table.index = summary_table.index + 1\n","summary_table.loc[0] = ['Intercept', reg_ead.intercept_]\n","summary_table = summary_table.sort_index()\n","p_values = reg_lgd_st_2.p\n","p_values = np.append(np.nan,np.array(p_values))\n","summary_table['p_values'] = p_values\n","summary_table"]},{"cell_type":"markdown","metadata":{"id":"4grV_MotDnsM"},"source":["### Model Validation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bY_cTkG9DnsM"},"outputs":[],"source":["ead_inputs_test = ead_inputs_test[features_all]\n","# Here we keep only the variables we need for the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gnTSKInbDnsM"},"outputs":[],"source":["ead_inputs_test = ead_inputs_test.drop(features_reference_cat, axis = 1)\n","# Here we remove the dummy variable reference categories."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pjYVpQ22DnsM"},"outputs":[],"source":["ead_inputs_test.columns.values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XZNoIjHxDnsN"},"outputs":[],"source":["y_hat_test_ead = reg_ead.predict(ead_inputs_test)\n","# Calculates the predicted values for the dependent variable (targets)\n","# based on the values of the independent variables (inputs) supplied as an argument."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_MmmxaL1DnsN"},"outputs":[],"source":["ead_targets_test_temp = ead_targets_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lFhljb8eDnsN"},"outputs":[],"source":["ead_targets_test_temp = ead_targets_test_temp.reset_index(drop = True)\n","# We reset the index of a dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AgvMOPrNDnsN"},"outputs":[],"source":["pd.concat([ead_targets_test_temp, pd.DataFrame(y_hat_test_ead)], axis = 1).corr()\n","# We calculate the correlation between actual and predicted values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eR_CK5pIDnsN"},"outputs":[],"source":["sns.distplot(ead_targets_test - y_hat_test_ead)\n","# We plot the distribution of the residuals."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2x9Jz5u5DnsO"},"outputs":[],"source":["pd.DataFrame(y_hat_test_ead).describe()\n","# Shows some descriptive statisics for the values of a column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0jZKL3-5DnsO"},"outputs":[],"source":["y_hat_test_ead = np.where(y_hat_test_ead < 0, 0, y_hat_test_ead)\n","y_hat_test_ead = np.where(y_hat_test_ead > 1, 1, y_hat_test_ead)\n","# We set predicted values that are greater than 1 to 1 and predicted values that are less than 0 to 0."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2sPYa7MjDnsO"},"outputs":[],"source":["pd.DataFrame(y_hat_test_ead).describe()\n","# Shows some descriptive statisics for the values of a column."]},{"cell_type":"markdown","metadata":{"id":"IednjT2FDnsO"},"source":["# Expected Loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pVhiQjctDnsO"},"outputs":[],"source":["loan_data_preprocessed.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SLdndWCMDnsP"},"outputs":[],"source":["loan_data_preprocessed['mths_since_last_delinq'].fillna(0, inplace = True)\n","# We fill the missing values with zeroes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4TuksWGbDnsP"},"outputs":[],"source":["loan_data_preprocessed['mths_since_last_record'].fillna(0, inplace = True)\n","# We fill the missing values with zeroes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q4lpOh3JDnsP"},"outputs":[],"source":["loan_data_preprocessed_lgd_ead = loan_data_preprocessed[features_all]\n","# Here we keep only the variables we need for the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9t8PB7E2DnsP"},"outputs":[],"source":["loan_data_preprocessed_lgd_ead = loan_data_preprocessed_lgd_ead.drop(features_reference_cat, axis = 1)\n","# Here we remove the dummy variable reference categories."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XIbBl0NFDnsQ"},"outputs":[],"source":["loan_data_preprocessed['recovery_rate_st_1'] = reg_lgd_st_1.model.predict(loan_data_preprocessed_lgd_ead)\n","# We apply the stage 1 LGD model and calculate predicted values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2bafXAdHDnsQ"},"outputs":[],"source":["loan_data_preprocessed['recovery_rate_st_2'] = reg_lgd_st_2.predict(loan_data_preprocessed_lgd_ead)\n","# We apply the stage 2 LGD model and calculate predicted values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d-94mMpEDnsQ"},"outputs":[],"source":["loan_data_preprocessed['recovery_rate'] = loan_data_preprocessed['recovery_rate_st_1'] * loan_data_preprocessed['recovery_rate_st_2']\n","# We combine the predicted values from the stage 1 predicted model and the stage 2 predicted model\n","# to calculate the final estimated recovery rate."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_FEdT95kDnsQ"},"outputs":[],"source":["loan_data_preprocessed['recovery_rate'] = np.where(loan_data_preprocessed['recovery_rate'] < 0, 0, loan_data_preprocessed['recovery_rate'])\n","loan_data_preprocessed['recovery_rate'] = np.where(loan_data_preprocessed['recovery_rate'] > 1, 1, loan_data_preprocessed['recovery_rate'])\n","# We set estimated recovery rates that are greater than 1 to 1 and  estimated recovery rates that are less than 0 to 0."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uldik5c_DnsQ"},"outputs":[],"source":["loan_data_preprocessed['LGD'] = 1 - loan_data_preprocessed['recovery_rate']\n","# We calculate estimated LGD. Estimated LGD equals 1 - estimated recovery rate."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ynmGp9cFDnsR"},"outputs":[],"source":["loan_data_preprocessed['LGD'].describe()\n","# Shows some descriptive statisics for the values of a column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WlaqK4xxDnsR"},"outputs":[],"source":["loan_data_preprocessed['CCF'] = reg_ead.predict(loan_data_preprocessed_lgd_ead)\n","# We apply the EAD model to calculate estimated credit conversion factor."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jgi5niduDnsR"},"outputs":[],"source":["loan_data_preprocessed['CCF'] = np.where(loan_data_preprocessed['CCF'] < 0, 0, loan_data_preprocessed['CCF'])\n","loan_data_preprocessed['CCF'] = np.where(loan_data_preprocessed['CCF'] > 1, 1, loan_data_preprocessed['CCF'])\n","# We set estimated CCF that are greater than 1 to 1 and  estimated CCF that are less than 0 to 0."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sduYWSryDnsR"},"outputs":[],"source":["loan_data_preprocessed['EAD'] = loan_data_preprocessed['CCF'] * loan_data_preprocessed_lgd_ead['funded_amnt']\n","# We calculate estimated EAD. Estimated EAD equals estimated CCF multiplied by funded amount."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M59D1eicDnsR"},"outputs":[],"source":["loan_data_preprocessed['EAD'].describe()\n","# Shows some descriptive statisics for the values of a column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8YQDm7KMDnsS"},"outputs":[],"source":["loan_data_preprocessed.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eSvqqGEzDnsS"},"outputs":[],"source":["loan_data_inputs_train = pd.read_csv('loan_data_inputs_train.csv')\n","# We import data to apply the PD model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"23xTB0spDnsS"},"outputs":[],"source":["loan_data_inputs_test = pd.read_csv('loan_data_inputs_test.csv')\n","# We import data to apply the PD model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C7wIIjsADnsS"},"outputs":[],"source":["loan_data_inputs_pd = pd.concat([loan_data_inputs_train, loan_data_inputs_test], axis = 0)\n","# We concatenate the two dataframes along the rows."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oiKSCJdIDnsS"},"outputs":[],"source":["loan_data_inputs_pd.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y9brnhjJDnsT"},"outputs":[],"source":["loan_data_inputs_pd.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Er4eX3NUDnsT"},"outputs":[],"source":["loan_data_inputs_pd = loan_data_inputs_pd.set_index('Unnamed: 0')\n","# We set the index of the dataframe to the values of a specific column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y01xLTQdDnsT"},"outputs":[],"source":["loan_data_inputs_pd.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L3vSHxTxDnsT"},"outputs":[],"source":["features_all_pd = ['grade:A',\n","'grade:B',\n","'grade:C',\n","'grade:D',\n","'grade:E',\n","'grade:F',\n","'grade:G',\n","'home_ownership:RENT_OTHER_NONE_ANY',\n","'home_ownership:OWN',\n","'home_ownership:MORTGAGE',\n","'addr_state:ND_NE_IA_NV_FL_HI_AL',\n","'addr_state:NM_VA',\n","'addr_state:NY',\n","'addr_state:OK_TN_MO_LA_MD_NC',\n","'addr_state:CA',\n","'addr_state:UT_KY_AZ_NJ',\n","'addr_state:AR_MI_PA_OH_MN',\n","'addr_state:RI_MA_DE_SD_IN',\n","'addr_state:GA_WA_OR',\n","'addr_state:WI_MT',\n","'addr_state:TX',\n","'addr_state:IL_CT',\n","'addr_state:KS_SC_CO_VT_AK_MS',\n","'addr_state:WV_NH_WY_DC_ME_ID',\n","'verification_status:Not Verified',\n","'verification_status:Source Verified',\n","'verification_status:Verified',\n","'purpose:educ__sm_b__wedd__ren_en__mov__house',\n","'purpose:credit_card',\n","'purpose:debt_consolidation',\n","'purpose:oth__med__vacation',\n","'purpose:major_purch__car__home_impr',\n","'initial_list_status:f',\n","'initial_list_status:w',\n","'term:36',\n","'term:60',\n","'emp_length:0',\n","'emp_length:1',\n","'emp_length:2-4',\n","'emp_length:5-6',\n","'emp_length:7-9',\n","'emp_length:10',\n","'mths_since_issue_d:<38',\n","'mths_since_issue_d:38-39',\n","'mths_since_issue_d:40-41',\n","'mths_since_issue_d:42-48',\n","'mths_since_issue_d:49-52',\n","'mths_since_issue_d:53-64',\n","'mths_since_issue_d:65-84',\n","'mths_since_issue_d:>84',\n","'int_rate:<9.548',\n","'int_rate:9.548-12.025',\n","'int_rate:12.025-15.74',\n","'int_rate:15.74-20.281',\n","'int_rate:>20.281',\n","'mths_since_earliest_cr_line:<140',\n","'mths_since_earliest_cr_line:141-164',\n","'mths_since_earliest_cr_line:165-247',\n","'mths_since_earliest_cr_line:248-270',\n","'mths_since_earliest_cr_line:271-352',\n","'mths_since_earliest_cr_line:>352',\n","'inq_last_6mths:0',\n","'inq_last_6mths:1-2',\n","'inq_last_6mths:3-6',\n","'inq_last_6mths:>6',\n","'acc_now_delinq:0',\n","'acc_now_delinq:>=1',\n","'annual_inc:<20K',\n","'annual_inc:20K-30K',\n","'annual_inc:30K-40K',\n","'annual_inc:40K-50K',\n","'annual_inc:50K-60K',\n","'annual_inc:60K-70K',\n","'annual_inc:70K-80K',\n","'annual_inc:80K-90K',\n","'annual_inc:90K-100K',\n","'annual_inc:100K-120K',\n","'annual_inc:120K-140K',\n","'annual_inc:>140K',\n","'dti:<=1.4',\n","'dti:1.4-3.5',\n","'dti:3.5-7.7',\n","'dti:7.7-10.5',\n","'dti:10.5-16.1',\n","'dti:16.1-20.3',\n","'dti:20.3-21.7',\n","'dti:21.7-22.4',\n","'dti:22.4-35',\n","'dti:>35',\n","'mths_since_last_delinq:Missing',\n","'mths_since_last_delinq:0-3',\n","'mths_since_last_delinq:4-30',\n","'mths_since_last_delinq:31-56',\n","'mths_since_last_delinq:>=57',\n","'mths_since_last_record:Missing',\n","'mths_since_last_record:0-2',\n","'mths_since_last_record:3-20',\n","'mths_since_last_record:21-31',\n","'mths_since_last_record:32-80',\n","'mths_since_last_record:81-86',\n","'mths_since_last_record:>=86']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D0Q3viW1DnsU"},"outputs":[],"source":["ref_categories_pd = ['grade:G',\n","'home_ownership:RENT_OTHER_NONE_ANY',\n","'addr_state:ND_NE_IA_NV_FL_HI_AL',\n","'verification_status:Verified',\n","'purpose:educ__sm_b__wedd__ren_en__mov__house',\n","'initial_list_status:f',\n","'term:60',\n","'emp_length:0',\n","'mths_since_issue_d:>84',\n","'int_rate:>20.281',\n","'mths_since_earliest_cr_line:<140',\n","'inq_last_6mths:>6',\n","'acc_now_delinq:0',\n","'annual_inc:<20K',\n","'dti:>35',\n","'mths_since_last_delinq:0-3',\n","'mths_since_last_record:0-2']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4OfzwoaxDnsU"},"outputs":[],"source":["loan_data_inputs_pd_temp = loan_data_inputs_pd[features_all_pd]\n","# Here we keep only the variables we need for the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G542rT8BDnsU"},"outputs":[],"source":["loan_data_inputs_pd_temp = loan_data_inputs_pd_temp.drop(ref_categories_pd, axis = 1)\n","# Here we remove the dummy variable reference categories."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WsFqyTmsDnsU"},"outputs":[],"source":["loan_data_inputs_pd_temp.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v8JfkggXDnsU"},"outputs":[],"source":["import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rGIE7TeKDnsV"},"outputs":[],"source":["reg_pd = pickle.load(open('pd_model.sav', 'rb'))\n","# We import the PD model, stored in the 'pd_model.sav' file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iSCIxBJ_DnsV"},"outputs":[],"source":["reg_pd.model.predict_proba(loan_data_inputs_pd_temp)[: ][: , 0]\n","# We apply the PD model to caclulate estimated default probabilities."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tDlpjvQ6DnsV"},"outputs":[],"source":["loan_data_inputs_pd['PD'] = reg_pd.model.predict_proba(loan_data_inputs_pd_temp)[: ][: , 0]\n","# We apply the PD model to caclulate estimated default probabilities."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-9_ozxQVDnsV"},"outputs":[],"source":["loan_data_inputs_pd['PD'].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AIsDxNISDnsV"},"outputs":[],"source":["loan_data_inputs_pd['PD'].describe()\n","# Shows some descriptive statisics for the values of a column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-XU_NOGMDnsV"},"outputs":[],"source":["loan_data_preprocessed_new = pd.concat([loan_data_preprocessed, loan_data_inputs_pd], axis = 1)\n","# We concatenate the dataframes where we calculated LGD and EAD and the dataframe where we calculated PD along the columns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k8TdmFH5DnsW"},"outputs":[],"source":["loan_data_preprocessed_new.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V51-bIyEDnsW"},"outputs":[],"source":["loan_data_preprocessed_new.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oW6W1GVVDnsW"},"outputs":[],"source":["loan_data_preprocessed_new['EL'] = loan_data_preprocessed_new['PD'] * loan_data_preprocessed_new['LGD'] * loan_data_preprocessed_new['EAD']\n","# We calculate Expected Loss. EL = PD * LGD * EAD."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J9Z67eGLDnsW"},"outputs":[],"source":["loan_data_preprocessed_new['EL'].describe()\n","# Shows some descriptive statisics for the values of a column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q2YcsBZTDnsW"},"outputs":[],"source":["loan_data_preprocessed_new[['funded_amnt', 'PD', 'LGD', 'EAD', 'EL']].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D_whEqFeDnsX"},"outputs":[],"source":["loan_data_preprocessed_new['funded_amnt'].describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ekHtefbDnsX"},"outputs":[],"source":["loan_data_preprocessed_new['EL'].sum()\n","# Total Expected Loss for all loans."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M0rcdartDnsX"},"outputs":[],"source":["loan_data_preprocessed_new['funded_amnt'].sum()\n","# Total funded amount for all loans."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"BtPCWqXHDnsX"},"outputs":[],"source":["loan_data_preprocessed_new['EL'].sum() / loan_data_preprocessed_new['funded_amnt'].sum()\n","# Total Expected Loss as a proportion of total funded amount for all loans.\n","####\n","####\n","####\n","# THE END."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}