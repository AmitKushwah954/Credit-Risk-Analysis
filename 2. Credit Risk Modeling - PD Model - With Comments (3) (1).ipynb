{"cells":[{"cell_type":"markdown","metadata":{"id":"PXU_8AoGAB3K"},"source":["# Import Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"H1c2VtRaAB3L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757573189288,"user_tz":-330,"elapsed":49471,"user":{"displayName":"Amit Kushwah","userId":"16953108842813831951"}},"outputId":"7637b540-44c6-443f-8bd7-29f87f9c664e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"9j7PFExDAB3M"},"source":["# Loading the Data and Selecting the Features"]},{"cell_type":"markdown","metadata":{"id":"8LfGYsHzAB3M"},"source":["### Import Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gqEwFp6VAB3N"},"outputs":[],"source":["loan_data_inputs_train = pd.read_csv('loan_data_inputs_train.csv', index_col = 0)\n","loan_data_targets_train = pd.read_csv('loan_data_targets_train.csv', index_col = 0, header = None)\n","loan_data_inputs_test = pd.read_csv('loan_data_inputs_test.csv', index_col = 0)\n","loan_data_targets_test = pd.read_csv('loan_data_targets_test.csv', index_col = 0, header = None)"]},{"cell_type":"markdown","metadata":{"id":"3LLK4CGTAB3N"},"source":["### Explore Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iYV7KetGAB3O"},"outputs":[],"source":["loan_data_inputs_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U3SyOTIXAB3O"},"outputs":[],"source":["loan_data_targets_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dw_1rzaAB3O"},"outputs":[],"source":["loan_data_inputs_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k3JzDGsoAB3P"},"outputs":[],"source":["loan_data_targets_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nOMruModAB3P"},"outputs":[],"source":["loan_data_inputs_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wc55ht8XAB3P"},"outputs":[],"source":["loan_data_targets_test.shape"]},{"cell_type":"markdown","metadata":{"id":"6WroE1irAB3P"},"source":["### Selecting the Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HWeLG57iAB3Q"},"outputs":[],"source":["# Here we select a limited set of input variables in a new dataframe.\n","inputs_train_with_ref_cat = loan_data_inputs_train.loc[: , ['grade:A',\n","'grade:B',\n","'grade:C',\n","'grade:D',\n","'grade:E',\n","'grade:F',\n","'grade:G',\n","'home_ownership:RENT_OTHER_NONE_ANY',\n","'home_ownership:OWN',\n","'home_ownership:MORTGAGE',\n","'addr_state:ND_NE_IA_NV_FL_HI_AL',\n","'addr_state:NM_VA',\n","'addr_state:NY',\n","'addr_state:OK_TN_MO_LA_MD_NC',\n","'addr_state:CA',\n","'addr_state:UT_KY_AZ_NJ',\n","'addr_state:AR_MI_PA_OH_MN',\n","'addr_state:RI_MA_DE_SD_IN',\n","'addr_state:GA_WA_OR',\n","'addr_state:WI_MT',\n","'addr_state:TX',\n","'addr_state:IL_CT',\n","'addr_state:KS_SC_CO_VT_AK_MS',\n","'addr_state:WV_NH_WY_DC_ME_ID',\n","'verification_status:Not Verified',\n","'verification_status:Source Verified',\n","'verification_status:Verified',\n","'purpose:educ__sm_b__wedd__ren_en__mov__house',\n","'purpose:credit_card',\n","'purpose:debt_consolidation',\n","'purpose:oth__med__vacation',\n","'purpose:major_purch__car__home_impr',\n","'initial_list_status:f',\n","'initial_list_status:w',\n","'term:36',\n","'term:60',\n","'emp_length:0',\n","'emp_length:1',\n","'emp_length:2-4',\n","'emp_length:5-6',\n","'emp_length:7-9',\n","'emp_length:10',\n","'mths_since_issue_d:<38',\n","'mths_since_issue_d:38-39',\n","'mths_since_issue_d:40-41',\n","'mths_since_issue_d:42-48',\n","'mths_since_issue_d:49-52',\n","'mths_since_issue_d:53-64',\n","'mths_since_issue_d:65-84',\n","'mths_since_issue_d:>84',\n","'int_rate:<9.548',\n","'int_rate:9.548-12.025',\n","'int_rate:12.025-15.74',\n","'int_rate:15.74-20.281',\n","'int_rate:>20.281',\n","'mths_since_earliest_cr_line:<140',\n","'mths_since_earliest_cr_line:141-164',\n","'mths_since_earliest_cr_line:165-247',\n","'mths_since_earliest_cr_line:248-270',\n","'mths_since_earliest_cr_line:271-352',\n","'mths_since_earliest_cr_line:>352',\n","'delinq_2yrs:0',\n","'delinq_2yrs:1-3',\n","'delinq_2yrs:>=4',\n","'inq_last_6mths:0',\n","'inq_last_6mths:1-2',\n","'inq_last_6mths:3-6',\n","'inq_last_6mths:>6',\n","'open_acc:0',\n","'open_acc:1-3',\n","'open_acc:4-12',\n","'open_acc:13-17',\n","'open_acc:18-22',\n","'open_acc:23-25',\n","'open_acc:26-30',\n","'open_acc:>=31',\n","'pub_rec:0-2',\n","'pub_rec:3-4',\n","'pub_rec:>=5',\n","'total_acc:<=27',\n","'total_acc:28-51',\n","'total_acc:>=52',\n","'acc_now_delinq:0',\n","'acc_now_delinq:>=1',\n","'total_rev_hi_lim:<=5K',\n","'total_rev_hi_lim:5K-10K',\n","'total_rev_hi_lim:10K-20K',\n","'total_rev_hi_lim:20K-30K',\n","'total_rev_hi_lim:30K-40K',\n","'total_rev_hi_lim:40K-55K',\n","'total_rev_hi_lim:55K-95K',\n","'total_rev_hi_lim:>95K',\n","'annual_inc:<20K',\n","'annual_inc:20K-30K',\n","'annual_inc:30K-40K',\n","'annual_inc:40K-50K',\n","'annual_inc:50K-60K',\n","'annual_inc:60K-70K',\n","'annual_inc:70K-80K',\n","'annual_inc:80K-90K',\n","'annual_inc:90K-100K',\n","'annual_inc:100K-120K',\n","'annual_inc:120K-140K',\n","'annual_inc:>140K',\n","'dti:<=1.4',\n","'dti:1.4-3.5',\n","'dti:3.5-7.7',\n","'dti:7.7-10.5',\n","'dti:10.5-16.1',\n","'dti:16.1-20.3',\n","'dti:20.3-21.7',\n","'dti:21.7-22.4',\n","'dti:22.4-35',\n","'dti:>35',\n","'mths_since_last_delinq:Missing',\n","'mths_since_last_delinq:0-3',\n","'mths_since_last_delinq:4-30',\n","'mths_since_last_delinq:31-56',\n","'mths_since_last_delinq:>=57',\n","'mths_since_last_record:Missing',\n","'mths_since_last_record:0-2',\n","'mths_since_last_record:3-20',\n","'mths_since_last_record:21-31',\n","'mths_since_last_record:32-80',\n","'mths_since_last_record:81-86',\n","'mths_since_last_record:>=86',\n","]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F9deTJJ_AB3Q"},"outputs":[],"source":["# Here we store the names of the reference category dummy variables in a list.\n","ref_categories = ['grade:G',\n","'home_ownership:RENT_OTHER_NONE_ANY',\n","'addr_state:ND_NE_IA_NV_FL_HI_AL',\n","'verification_status:Verified',\n","'purpose:educ__sm_b__wedd__ren_en__mov__house',\n","'initial_list_status:f',\n","'term:60',\n","'emp_length:0',\n","'mths_since_issue_d:>84',\n","'int_rate:>20.281',\n","'mths_since_earliest_cr_line:<140',\n","'delinq_2yrs:>=4',\n","'inq_last_6mths:>6',\n","'open_acc:0',\n","'pub_rec:0-2',\n","'total_acc:<=27',\n","'acc_now_delinq:0',\n","'total_rev_hi_lim:<=5K',\n","'annual_inc:<20K',\n","'dti:>35',\n","'mths_since_last_delinq:0-3',\n","'mths_since_last_record:0-2']"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"nnWgUPAzAB3Q"},"outputs":[],"source":["inputs_train = inputs_train_with_ref_cat.drop(ref_categories, axis = 1)\n","# From the dataframe with input variables, we drop the variables with variable names in the list with reference categories.\n","inputs_train.head()"]},{"cell_type":"markdown","metadata":{"id":"w9JgHXMDAB3R"},"source":["# PD Model Estimation"]},{"cell_type":"markdown","metadata":{"id":"LW2SRUxQAB3R"},"source":["## Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PMUXOW_KAB3R"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn import metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QJMPr1eRAB3R"},"outputs":[],"source":["reg = LogisticRegression()\n","# We create an instance of an object from the 'LogisticRegression' class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_rnD40MTAB3R"},"outputs":[],"source":["pd.options.display.max_rows = None\n","# Sets the pandas dataframe options to display all columns/ rows."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"AtIbR7SiAB3R"},"outputs":[],"source":["reg.fit(inputs_train, loan_data_targets_train)\n","# Estimates the coefficients of the object from the 'LogisticRegression' class\n","# with inputs (independent variables) contained in the first dataframe\n","# and targets (dependent variables) contained in the second dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yanMt_4hAB3R"},"outputs":[],"source":["reg.intercept_\n","# Displays the intercept contain in the estimated (\"fitted\") object from the 'LogisticRegression' class."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"-6u3T5qwAB3R"},"outputs":[],"source":["reg.coef_\n","# Displays the coefficients contained in the estimated (\"fitted\") object from the 'LogisticRegression' class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CH-_Dpy0AB3R"},"outputs":[],"source":["feature_name = inputs_train.columns.values\n","# Stores the names of the columns of a dataframe in a variable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ABf4PRkWAB3R"},"outputs":[],"source":["summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n","# Creates a dataframe with a column titled 'Feature name' and row values contained in the 'feature_name' variable.\n","summary_table['Coefficients'] = np.transpose(reg.coef_)\n","# Creates a new column in the dataframe, called 'Coefficients',\n","# with row values the transposed coefficients from the 'LogisticRegression' object.\n","summary_table.index = summary_table.index + 1\n","# Increases the index of every row of the dataframe with 1.\n","summary_table.loc[0] = ['Intercept', reg.intercept_[0]]\n","# Assigns values of the row with index 0 of the dataframe.\n","summary_table = summary_table.sort_index()\n","# Sorts the dataframe by index.\n","summary_table"]},{"cell_type":"markdown","metadata":{"id":"y6Q6U_UPAB3S"},"source":["## Build a Logistic Regression Model with P-Values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F306_bYYAB3S"},"outputs":[],"source":["# P values for sklearn logistic regression.\n","\n","# Class to display p-values for logistic regression in sklearn.\n","\n","from sklearn import linear_model\n","import scipy.stats as stat\n","\n","class LogisticRegression_with_p_values:\n","\n","    def __init__(self,*args,**kwargs):#,**kwargs):\n","        self.model = linear_model.LogisticRegression(*args,**kwargs)#,**args)\n","\n","    def fit(self,X,y):\n","        self.model.fit(X,y)\n","\n","        #### Get p-values for the fitted model ####\n","        denom = (2.0 * (1.0 + np.cosh(self.model.decision_function(X))))\n","        denom = np.tile(denom,(X.shape[1],1)).T\n","        F_ij = np.dot((X / denom).T,X) ## Fisher Information Matrix\n","        Cramer_Rao = np.linalg.inv(F_ij) ## Inverse Information Matrix\n","        sigma_estimates = np.sqrt(np.diagonal(Cramer_Rao))\n","        z_scores = self.model.coef_[0] / sigma_estimates # z-score for eaach model coefficient\n","        p_values = [stat.norm.sf(abs(x)) * 2 for x in z_scores] ### two tailed test for p-values\n","\n","        self.coef_ = self.model.coef_\n","        self.intercept_ = self.model.intercept_\n","        #self.z_scores = z_scores\n","        self.p_values = p_values\n","        #self.sigma_estimates = sigma_estimates\n","        #self.F_ij = F_ij"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h2dNiy2lAB3S"},"outputs":[],"source":["from sklearn import linear_model\n","import scipy.stats as stat\n","\n","class LogisticRegression_with_p_values:\n","\n","    def __init__(self,*args,**kwargs):\n","        self.model = linear_model.LogisticRegression(*args,**kwargs)\n","\n","    def fit(self,X,y):\n","        self.model.fit(X,y)\n","        denom = (2.0 * (1.0 + np.cosh(self.model.decision_function(X))))\n","        denom = np.tile(denom,(X.shape[1],1)).T\n","        F_ij = np.dot((X / denom).T,X)\n","        Cramer_Rao = np.linalg.inv(F_ij)\n","        sigma_estimates = np.sqrt(np.diagonal(Cramer_Rao))\n","        z_scores = self.model.coef_[0] / sigma_estimates\n","        p_values = [stat.norm.sf(abs(x)) * 2 for x in z_scores]\n","        self.coef_ = self.model.coef_\n","        self.intercept_ = self.model.intercept_\n","        self.p_values = p_values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qgRSkjJTAB3S"},"outputs":[],"source":["reg = LogisticRegression_with_p_values()\n","# We create an instance of an object from the newly created 'LogisticRegression_with_p_values()' class."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"KTV9op-aAB3S"},"outputs":[],"source":["reg.fit(inputs_train, loan_data_targets_train)\n","# Estimates the coefficients of the object from the 'LogisticRegression' class\n","# with inputs (independent variables) contained in the first dataframe\n","# and targets (dependent variables) contained in the second dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tcl-UBU2AB3S"},"outputs":[],"source":["# Same as above.\n","summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n","summary_table['Coefficients'] = np.transpose(reg.coef_)\n","summary_table.index = summary_table.index + 1\n","summary_table.loc[0] = ['Intercept', reg.intercept_[0]]\n","summary_table = summary_table.sort_index()\n","summary_table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"84n4YDiJAB3S"},"outputs":[],"source":["# This is a list.\n","p_values = reg.p_values\n","# We take the result of the newly added method 'p_values' and store it in a variable 'p_values'."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ooRDmibNAB3S"},"outputs":[],"source":["# Add the intercept for completeness.\n","p_values = np.append(np.nan, np.array(p_values))\n","# We add the value 'NaN' in the beginning of the variable with p-values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SqrGUw8hAB3T"},"outputs":[],"source":["summary_table['p_values'] = p_values\n","# In the 'summary_table' dataframe, we add a new column, called 'p_values', containing the values from the 'p_values' variable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AHfHsY3qAB3T"},"outputs":[],"source":["summary_table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dhCaqpVGAB3T"},"outputs":[],"source":["# We are going to remove some features, the coefficients for all or almost all of the dummy variables for which,\n","# are not tatistically significant.\n","\n","# We do that by specifying another list of dummy variables as reference categories, and a list of variables to remove.\n","# Then, we are going to drop the two datasets from the original list of dummy variables.\n","\n","# Variables\n","inputs_train_with_ref_cat = loan_data_inputs_train.loc[: , ['grade:A',\n","'grade:B',\n","'grade:C',\n","'grade:D',\n","'grade:E',\n","'grade:F',\n","'grade:G',\n","'home_ownership:RENT_OTHER_NONE_ANY',\n","'home_ownership:OWN',\n","'home_ownership:MORTGAGE',\n","'addr_state:ND_NE_IA_NV_FL_HI_AL',\n","'addr_state:NM_VA',\n","'addr_state:NY',\n","'addr_state:OK_TN_MO_LA_MD_NC',\n","'addr_state:CA',\n","'addr_state:UT_KY_AZ_NJ',\n","'addr_state:AR_MI_PA_OH_MN',\n","'addr_state:RI_MA_DE_SD_IN',\n","'addr_state:GA_WA_OR',\n","'addr_state:WI_MT',\n","'addr_state:TX',\n","'addr_state:IL_CT',\n","'addr_state:KS_SC_CO_VT_AK_MS',\n","'addr_state:WV_NH_WY_DC_ME_ID',\n","'verification_status:Not Verified',\n","'verification_status:Source Verified',\n","'verification_status:Verified',\n","'purpose:educ__sm_b__wedd__ren_en__mov__house',\n","'purpose:credit_card',\n","'purpose:debt_consolidation',\n","'purpose:oth__med__vacation',\n","'purpose:major_purch__car__home_impr',\n","'initial_list_status:f',\n","'initial_list_status:w',\n","'term:36',\n","'term:60',\n","'emp_length:0',\n","'emp_length:1',\n","'emp_length:2-4',\n","'emp_length:5-6',\n","'emp_length:7-9',\n","'emp_length:10',\n","'mths_since_issue_d:<38',\n","'mths_since_issue_d:38-39',\n","'mths_since_issue_d:40-41',\n","'mths_since_issue_d:42-48',\n","'mths_since_issue_d:49-52',\n","'mths_since_issue_d:53-64',\n","'mths_since_issue_d:65-84',\n","'mths_since_issue_d:>84',\n","'int_rate:<9.548',\n","'int_rate:9.548-12.025',\n","'int_rate:12.025-15.74',\n","'int_rate:15.74-20.281',\n","'int_rate:>20.281',\n","'mths_since_earliest_cr_line:<140',\n","'mths_since_earliest_cr_line:141-164',\n","'mths_since_earliest_cr_line:165-247',\n","'mths_since_earliest_cr_line:248-270',\n","'mths_since_earliest_cr_line:271-352',\n","'mths_since_earliest_cr_line:>352',\n","'inq_last_6mths:0',\n","'inq_last_6mths:1-2',\n","'inq_last_6mths:3-6',\n","'inq_last_6mths:>6',\n","'acc_now_delinq:0',\n","'acc_now_delinq:>=1',\n","'annual_inc:<20K',\n","'annual_inc:20K-30K',\n","'annual_inc:30K-40K',\n","'annual_inc:40K-50K',\n","'annual_inc:50K-60K',\n","'annual_inc:60K-70K',\n","'annual_inc:70K-80K',\n","'annual_inc:80K-90K',\n","'annual_inc:90K-100K',\n","'annual_inc:100K-120K',\n","'annual_inc:120K-140K',\n","'annual_inc:>140K',\n","'dti:<=1.4',\n","'dti:1.4-3.5',\n","'dti:3.5-7.7',\n","'dti:7.7-10.5',\n","'dti:10.5-16.1',\n","'dti:16.1-20.3',\n","'dti:20.3-21.7',\n","'dti:21.7-22.4',\n","'dti:22.4-35',\n","'dti:>35',\n","'mths_since_last_delinq:Missing',\n","'mths_since_last_delinq:0-3',\n","'mths_since_last_delinq:4-30',\n","'mths_since_last_delinq:31-56',\n","'mths_since_last_delinq:>=57',\n","'mths_since_last_record:Missing',\n","'mths_since_last_record:0-2',\n","'mths_since_last_record:3-20',\n","'mths_since_last_record:21-31',\n","'mths_since_last_record:32-80',\n","'mths_since_last_record:81-86',\n","'mths_since_last_record:>=86',\n","]]\n","\n","ref_categories = ['grade:G',\n","'home_ownership:RENT_OTHER_NONE_ANY',\n","'addr_state:ND_NE_IA_NV_FL_HI_AL',\n","'verification_status:Verified',\n","'purpose:educ__sm_b__wedd__ren_en__mov__house',\n","'initial_list_status:f',\n","'term:60',\n","'emp_length:0',\n","'mths_since_issue_d:>84',\n","'int_rate:>20.281',\n","'mths_since_earliest_cr_line:<140',\n","'inq_last_6mths:>6',\n","'acc_now_delinq:0',\n","'annual_inc:<20K',\n","'dti:>35',\n","'mths_since_last_delinq:0-3',\n","'mths_since_last_record:0-2']"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"i-5ADdhRAB3T"},"outputs":[],"source":["inputs_train = inputs_train_with_ref_cat.drop(ref_categories, axis = 1)\n","inputs_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d5wux2AAAB3T"},"outputs":[],"source":["# Here we run a new model.\n","reg2 = LogisticRegression_with_p_values()\n","reg2.fit(inputs_train, loan_data_targets_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hlr6-8FlAB3Y"},"outputs":[],"source":["feature_name = inputs_train.columns.values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JPsRWL_vAB3Y"},"outputs":[],"source":["# Same as above.\n","summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n","summary_table['Coefficients'] = np.transpose(reg2.coef_)\n","summary_table.index = summary_table.index + 1\n","summary_table.loc[0] = ['Intercept', reg2.intercept_[0]]\n","summary_table = summary_table.sort_index()\n","summary_table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RXwRK6RSAB3Y"},"outputs":[],"source":["# We add the 'p_values' here, just as we did before.\n","p_values = reg2.p_values\n","p_values = np.append(np.nan,np.array(p_values))\n","summary_table['p_values'] = p_values\n","summary_table\n","# Here we get the results for our final PD model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N7IxCbK9AB3Y"},"outputs":[],"source":["import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xDBGMGArAB3Y"},"outputs":[],"source":["pickle.dump(reg2, open('pd_model.sav', 'wb'))\n","# Here we export our model to a 'SAV' file with file name 'pd_model.sav'."]},{"cell_type":"markdown","metadata":{"id":"rkJfPf2yAB3Z"},"source":["# PD Model Validation (Test)"]},{"cell_type":"markdown","metadata":{"id":"DGVxXaFdAB3Z"},"source":["### Out-of-sample validation (test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JbdTfr_QAB3Z"},"outputs":[],"source":["# Here, from the dataframe with inputs for testing, we keep the same variables that we used in our final PD model.\n","inputs_test_with_ref_cat = loan_data_inputs_test.loc[: , ['grade:A',\n","'grade:B',\n","'grade:C',\n","'grade:D',\n","'grade:E',\n","'grade:F',\n","'grade:G',\n","'home_ownership:RENT_OTHER_NONE_ANY',\n","'home_ownership:OWN',\n","'home_ownership:MORTGAGE',\n","'addr_state:ND_NE_IA_NV_FL_HI_AL',\n","'addr_state:NM_VA',\n","'addr_state:NY',\n","'addr_state:OK_TN_MO_LA_MD_NC',\n","'addr_state:CA',\n","'addr_state:UT_KY_AZ_NJ',\n","'addr_state:AR_MI_PA_OH_MN',\n","'addr_state:RI_MA_DE_SD_IN',\n","'addr_state:GA_WA_OR',\n","'addr_state:WI_MT',\n","'addr_state:TX',\n","'addr_state:IL_CT',\n","'addr_state:KS_SC_CO_VT_AK_MS',\n","'addr_state:WV_NH_WY_DC_ME_ID',\n","'verification_status:Not Verified',\n","'verification_status:Source Verified',\n","'verification_status:Verified',\n","'purpose:educ__sm_b__wedd__ren_en__mov__house',\n","'purpose:credit_card',\n","'purpose:debt_consolidation',\n","'purpose:oth__med__vacation',\n","'purpose:major_purch__car__home_impr',\n","'initial_list_status:f',\n","'initial_list_status:w',\n","'term:36',\n","'term:60',\n","'emp_length:0',\n","'emp_length:1',\n","'emp_length:2-4',\n","'emp_length:5-6',\n","'emp_length:7-9',\n","'emp_length:10',\n","'mths_since_issue_d:<38',\n","'mths_since_issue_d:38-39',\n","'mths_since_issue_d:40-41',\n","'mths_since_issue_d:42-48',\n","'mths_since_issue_d:49-52',\n","'mths_since_issue_d:53-64',\n","'mths_since_issue_d:65-84',\n","'mths_since_issue_d:>84',\n","'int_rate:<9.548',\n","'int_rate:9.548-12.025',\n","'int_rate:12.025-15.74',\n","'int_rate:15.74-20.281',\n","'int_rate:>20.281',\n","'mths_since_earliest_cr_line:<140',\n","'mths_since_earliest_cr_line:141-164',\n","'mths_since_earliest_cr_line:165-247',\n","'mths_since_earliest_cr_line:248-270',\n","'mths_since_earliest_cr_line:271-352',\n","'mths_since_earliest_cr_line:>352',\n","'inq_last_6mths:0',\n","'inq_last_6mths:1-2',\n","'inq_last_6mths:3-6',\n","'inq_last_6mths:>6',\n","'acc_now_delinq:0',\n","'acc_now_delinq:>=1',\n","'annual_inc:<20K',\n","'annual_inc:20K-30K',\n","'annual_inc:30K-40K',\n","'annual_inc:40K-50K',\n","'annual_inc:50K-60K',\n","'annual_inc:60K-70K',\n","'annual_inc:70K-80K',\n","'annual_inc:80K-90K',\n","'annual_inc:90K-100K',\n","'annual_inc:100K-120K',\n","'annual_inc:120K-140K',\n","'annual_inc:>140K',\n","'dti:<=1.4',\n","'dti:1.4-3.5',\n","'dti:3.5-7.7',\n","'dti:7.7-10.5',\n","'dti:10.5-16.1',\n","'dti:16.1-20.3',\n","'dti:20.3-21.7',\n","'dti:21.7-22.4',\n","'dti:22.4-35',\n","'dti:>35',\n","'mths_since_last_delinq:Missing',\n","'mths_since_last_delinq:0-3',\n","'mths_since_last_delinq:4-30',\n","'mths_since_last_delinq:31-56',\n","'mths_since_last_delinq:>=57',\n","'mths_since_last_record:Missing',\n","'mths_since_last_record:0-2',\n","'mths_since_last_record:3-20',\n","'mths_since_last_record:21-31',\n","'mths_since_last_record:32-80',\n","'mths_since_last_record:81-86',\n","'mths_since_last_record:>=86',\n","]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IgegxHDgAB3Z"},"outputs":[],"source":["# And here, in the list below, we keep the variable names for the reference categories,\n","# only for the variables we used in our final PD model.\n","ref_categories = ['grade:G',\n","'home_ownership:RENT_OTHER_NONE_ANY',\n","'addr_state:ND_NE_IA_NV_FL_HI_AL',\n","'verification_status:Verified',\n","'purpose:educ__sm_b__wedd__ren_en__mov__house',\n","'initial_list_status:f',\n","'term:60',\n","'emp_length:0',\n","'mths_since_issue_d:>84',\n","'int_rate:>20.281',\n","'mths_since_earliest_cr_line:<140',\n","'inq_last_6mths:>6',\n","'acc_now_delinq:0',\n","'annual_inc:<20K',\n","'dti:>35',\n","'mths_since_last_delinq:0-3',\n","'mths_since_last_record:0-2']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p1Q3V5n8AB3Z"},"outputs":[],"source":["inputs_test = inputs_test_with_ref_cat.drop(ref_categories, axis = 1)\n","inputs_test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fJYYiMwbAB3Z"},"outputs":[],"source":["y_hat_test = reg2.model.predict(inputs_test)\n","# Calculates the predicted values for the dependent variable (targets)\n","# based on the values of the independent variables (inputs) supplied as an argument."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G_GY0JkDAB3Z"},"outputs":[],"source":["y_hat_test\n","# This is an array of predicted discrete classess (in this case, 0s and 1s)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gChC9uu8AB3Z"},"outputs":[],"source":["y_hat_test_proba = reg2.model.predict_proba(inputs_test)\n","# Calculates the predicted probability values for the dependent variable (targets)\n","# based on the values of the independent variables (inputs) supplied as an argument."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gg7AtvKhAB3a"},"outputs":[],"source":["y_hat_test_proba\n","# This is an array of arrays of predicted class probabilities for all classes.\n","# In this case, the first value of every sub-array is the probability for the observation to belong to the first class, i.e. 0,\n","# and the second value is the probability for the observation to belong to the first class, i.e. 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ux-iGNSMAB3a"},"outputs":[],"source":["y_hat_test_proba[:][:,1]\n","# Here we take all the arrays in the array, and from each array, we take all rows, and only the element with index 1,\n","# that is, the second element.\n","# In other words, we take only the probabilities for being 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vg4BKXCsAB3a"},"outputs":[],"source":["y_hat_test_proba = y_hat_test_proba[: ][: , 1]\n","# We store these probabilities in a variable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9j81TWHEAB3a"},"outputs":[],"source":["y_hat_test_proba\n","# This variable contains an array of probabilities of being 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-3OqxkvpAB3a"},"outputs":[],"source":["loan_data_targets_test_temp = loan_data_targets_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xwb1lFs9AB3a"},"outputs":[],"source":["loan_data_targets_test_temp.reset_index(drop = True, inplace = True)\n","# We reset the index of a dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7OHNfvrQAB3a"},"outputs":[],"source":["df_actual_predicted_probs = pd.concat([loan_data_targets_test_temp, pd.DataFrame(y_hat_test_proba)], axis = 1)\n","# Concatenates two dataframes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nnoGawuuAB3b"},"outputs":[],"source":["df_actual_predicted_probs.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K5kV7t7HAB3b"},"outputs":[],"source":["df_actual_predicted_probs.columns = ['loan_data_targets_test', 'y_hat_test_proba']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HmeIyJUKAB3b"},"outputs":[],"source":["df_actual_predicted_probs.index = loan_data_inputs_test.index\n","# Makes the index of one dataframe equal to the index of another dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cOoSRqGoAB3b"},"outputs":[],"source":["df_actual_predicted_probs.head()"]},{"cell_type":"markdown","metadata":{"id":"qQGHD_ADAB3b"},"source":["### Accuracy and Area under the Curve"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FndCBZiCAB3b"},"outputs":[],"source":["tr = 0.9\n","# We create a new column with an indicator,\n","# where every observation that has predicted probability greater than the threshold has a value of 1,\n","# and every observation that has predicted probability lower than the threshold has a value of 0.\n","df_actual_predicted_probs['y_hat_test'] = np.where(df_actual_predicted_probs['y_hat_test_proba'] > tr, 1, 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"XfgomNQ-AB3b"},"outputs":[],"source":["pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted'])\n","# Creates a cross-table where the actual values are displayed by rows and the predicted values by columns.\n","# This table is known as a Confusion Matrix."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ekSSYPWQAB3b"},"outputs":[],"source":["pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]\n","# Here we divide each value of the table by the total number of observations,\n","# thus getting percentages, or, rates."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SG8XrTZJAB3b"},"outputs":[],"source":["(pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[0, 0] + (pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[1, 1]\n","# Here we calculate Accuracy of the model, which is the sum of the diagonal rates."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xzWs7il2AB3c"},"outputs":[],"source":["from sklearn.metrics import roc_curve, roc_auc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0t5D7KIXAB3c"},"outputs":[],"source":["roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])\n","# Returns the Receiver Operating Characteristic (ROC) Curve from a set of actual values and their predicted probabilities.\n","# As a result, we get three arrays: the false positive rates, the true positive rates, and the thresholds."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OTyEn4YqAB3c"},"outputs":[],"source":["fpr, tpr, thresholds = roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])\n","# Here we store each of the three arrays in a separate variable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NbIr_np0AB3c"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set()"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"Fo3kDrDDAB3c"},"outputs":[],"source":["plt.plot(fpr, tpr)\n","# We plot the false positive rate along the x-axis and the true positive rate along the y-axis,\n","# thus plotting the ROC curve.\n","plt.plot(fpr, fpr, linestyle = '--', color = 'k')\n","# We plot a seconary diagonal line, with dashed line style and black color.\n","plt.xlabel('False positive rate')\n","# We name the x-axis \"False positive rate\".\n","plt.ylabel('True positive rate')\n","# We name the x-axis \"True positive rate\".\n","plt.title('ROC curve')\n","# We name the graph \"ROC curve\"."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"msZYLZhqAB3c"},"outputs":[],"source":["AUROC = roc_auc_score(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])\n","# Calculates the Area Under the Receiver Operating Characteristic Curve (AUROC)\n","# from a set of actual values and their predicted probabilities.\n","AUROC"]},{"cell_type":"markdown","metadata":{"id":"SF2Fu7cXAB3d"},"source":["### Gini and Kolmogorov-Smirnov"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EELasMrGAB3d"},"outputs":[],"source":["df_actual_predicted_probs = df_actual_predicted_probs.sort_values('y_hat_test_proba')\n","# Sorts a dataframe by the values of a specific column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qnPVUX2xAB3d"},"outputs":[],"source":["df_actual_predicted_probs.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o91FK2nBAB3d"},"outputs":[],"source":["df_actual_predicted_probs.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uzkfkzrqAB3d"},"outputs":[],"source":["df_actual_predicted_probs = df_actual_predicted_probs.reset_index()\n","# We reset the index of a dataframe and overwrite it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XSTFN4KDAB3d"},"outputs":[],"source":["df_actual_predicted_probs.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fjgpYf2eAB3d"},"outputs":[],"source":["df_actual_predicted_probs['Cumulative N Population'] = df_actual_predicted_probs.index + 1\n","# We calculate the cumulative number of all observations.\n","# We use the new index for that. Since indexing in ython starts from 0, we add 1 to each index.\n","df_actual_predicted_probs['Cumulative N Good'] = df_actual_predicted_probs['loan_data_targets_test'].cumsum()\n","# We calculate cumulative number of 'good', which is the cumulative sum of the column with actual observations.\n","df_actual_predicted_probs['Cumulative N Bad'] = df_actual_predicted_probs['Cumulative N Population'] - df_actual_predicted_probs['loan_data_targets_test'].cumsum()\n","# We calculate cumulative number of 'bad', which is\n","# the difference between the cumulative number of all observations and cumulative number of 'good' for each row."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qXrQSfXlAB3d"},"outputs":[],"source":["df_actual_predicted_probs.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZKn7zSsuAB3d"},"outputs":[],"source":["df_actual_predicted_probs['Cumulative Perc Population'] = df_actual_predicted_probs['Cumulative N Population'] / (df_actual_predicted_probs.shape[0])\n","# We calculate the cumulative percentage of all observations.\n","df_actual_predicted_probs['Cumulative Perc Good'] = df_actual_predicted_probs['Cumulative N Good'] / df_actual_predicted_probs['loan_data_targets_test'].sum()\n","# We calculate cumulative percentage of 'good'.\n","df_actual_predicted_probs['Cumulative Perc Bad'] = df_actual_predicted_probs['Cumulative N Bad'] / (df_actual_predicted_probs.shape[0] - df_actual_predicted_probs['loan_data_targets_test'].sum())\n","# We calculate the cumulative percentage of 'bad'."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b9ISZXG1AB3d"},"outputs":[],"source":["df_actual_predicted_probs.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hlg-4k3dAB3d"},"outputs":[],"source":["df_actual_predicted_probs.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"APvJEhaNAB3e"},"outputs":[],"source":["# Plot Gini\n","plt.plot(df_actual_predicted_probs['Cumulative Perc Population'], df_actual_predicted_probs['Cumulative Perc Bad'])\n","# We plot the cumulative percentage of all along the x-axis and the cumulative percentage 'good' along the y-axis,\n","# thus plotting the Gini curve.\n","plt.plot(df_actual_predicted_probs['Cumulative Perc Population'], df_actual_predicted_probs['Cumulative Perc Population'], linestyle = '--', color = 'k')\n","# We plot a seconary diagonal line, with dashed line style and black color.\n","plt.xlabel('Cumulative % Population')\n","# We name the x-axis \"Cumulative % Population\".\n","plt.ylabel('Cumulative % Bad')\n","# We name the y-axis \"Cumulative % Bad\".\n","plt.title('Gini')\n","# We name the graph \"Gini\"."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ivIC_InUAB3e"},"outputs":[],"source":["Gini = AUROC * 2 - 1\n","# Here we calculate Gini from AUROC.\n","Gini"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PZibnicSAB3e"},"outputs":[],"source":["# Plot KS\n","plt.plot(df_actual_predicted_probs['y_hat_test_proba'], df_actual_predicted_probs['Cumulative Perc Bad'], color = 'r')\n","# We plot the predicted (estimated) probabilities along the x-axis and the cumulative percentage 'bad' along the y-axis,\n","# colored in red.\n","plt.plot(df_actual_predicted_probs['y_hat_test_proba'], df_actual_predicted_probs['Cumulative Perc Good'], color = 'b')\n","# We plot the predicted (estimated) probabilities along the x-axis and the cumulative percentage 'good' along the y-axis,\n","# colored in red.\n","plt.xlabel('Estimated Probability for being Good')\n","# We name the x-axis \"Estimated Probability for being Good\".\n","plt.ylabel('Cumulative %')\n","# We name the y-axis \"Cumulative %\".\n","plt.title('Kolmogorov-Smirnov')\n","# We name the graph \"Kolmogorov-Smirnov\"."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MhWcphgbAB3e"},"outputs":[],"source":["KS = max(df_actual_predicted_probs['Cumulative Perc Bad'] - df_actual_predicted_probs['Cumulative Perc Good'])\n","# We calculate KS from the data. It is the maximum of the difference between the cumulative percentage of 'bad'\n","# and the cumulative percentage of 'good'.\n","KS"]},{"cell_type":"markdown","metadata":{"id":"zIfZwvjhAB3e"},"source":["# Applying the PD Model"]},{"cell_type":"markdown","metadata":{"id":"CxO_MJ8zAB3e"},"source":["### Calculating PD of individual accounts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6jgJ2L5PAB3e"},"outputs":[],"source":["pd.options.display.max_columns = None\n","# Sets the pandas dataframe options to display all columns/ rows."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WXLDGz7-AB3e"},"outputs":[],"source":["inputs_test_with_ref_cat.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_EoxE5RpAB3e"},"outputs":[],"source":["summary_table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ny-N-soOAB3f"},"outputs":[],"source":["y_hat_test_proba"]},{"cell_type":"markdown","metadata":{"id":"OwfKzuh0AB3f"},"source":["### Creating a Scorecard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LjPYb--mAB3f"},"outputs":[],"source":["summary_table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4FrZHzsjAB3f"},"outputs":[],"source":["ref_categories"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SGFG5-7vAB3f"},"outputs":[],"source":["df_ref_categories = pd.DataFrame(ref_categories, columns = ['Feature name'])\n","# We create a new dataframe with one column. Its values are the values from the 'reference_categories' list.\n","# We name it 'Feature name'.\n","df_ref_categories['Coefficients'] = 0\n","# We create a second column, called 'Coefficients', which contains only 0 values.\n","df_ref_categories['p_values'] = np.nan\n","# We create a third column, called 'p_values', with contains only NaN values.\n","df_ref_categories"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nXoGJW0AAB3f"},"outputs":[],"source":["df_scorecard = pd.concat([summary_table, df_ref_categories])\n","# Concatenates two dataframes.\n","df_scorecard = df_scorecard.reset_index()\n","# We reset the index of a dataframe.\n","df_scorecard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TZBwrKxNAB3f"},"outputs":[],"source":["df_scorecard['Original feature name'] = df_scorecard['Feature name'].str.split(':').str[0]\n","# We create a new column, called 'Original feature name', which contains the value of the 'Feature name' column,\n","# up to the column symbol.\n","df_scorecard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OOGBPOELAB3f"},"outputs":[],"source":["min_score = 300\n","max_score = 850"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BHv3BmGfAB3f"},"outputs":[],"source":["df_scorecard.groupby('Original feature name')['Coefficients'].min()\n","# Groups the data by the values of the 'Original feature name' column.\n","# Aggregates the data in the 'Coefficients' column, calculating their minimum."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_8vL0yisAB3g"},"outputs":[],"source":["min_sum_coef = df_scorecard.groupby('Original feature name')['Coefficients'].min().sum()\n","# Up to the 'min()' method everything is the same as in te line above.\n","# Then, we aggregate further and sum all the minimum values.\n","min_sum_coef"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"saYjnjJHAB3g"},"outputs":[],"source":["\n","df_scorecard.groupby('Original feature name')['Coefficients'].max()\n","# Groups the data by the values of the 'Original feature name' column.\n","# Aggregates the data in the 'Coefficients' column, calculating their maximum."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QIZJVPR2AB3g"},"outputs":[],"source":["max_sum_coef = df_scorecard.groupby('Original feature name')['Coefficients'].max().sum()\n","# Up to the 'min()' method everything is the same as in te line above.\n","# Then, we aggregate further and sum all the maximum values.\n","max_sum_coef"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GgaiXWboAB3g"},"outputs":[],"source":["df_scorecard['Score - Calculation'] = df_scorecard['Coefficients'] * (max_score - min_score) / (max_sum_coef - min_sum_coef)\n","# We multiply the value of the 'Coefficients' column by the ration of the differences between\n","# maximum score and minimum score and maximum sum of coefficients and minimum sum of cefficients.\n","df_scorecard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hfQyeb7BAB3g"},"outputs":[],"source":["df_scorecard['Score - Calculation'][0] = ((df_scorecard['Coefficients'][0] - min_sum_coef) / (max_sum_coef - min_sum_coef)) * (max_score - min_score) + min_score\n","# We divide the difference of the value of the 'Coefficients' column and the minimum sum of coefficients by\n","# the difference of the maximum sum of coefficients and the minimum sum of coefficients.\n","# Then, we multiply that by the difference between the maximum score and the minimum score.\n","# Then, we add minimum score.\n","df_scorecard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EzW8Jl0_AB3g"},"outputs":[],"source":["df_scorecard['Score - Preliminary'] = df_scorecard['Score - Calculation'].round()\n","# We round the values of the 'Score - Calculation' column.\n","df_scorecard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9T8gUHCrAB3g"},"outputs":[],"source":["min_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Preliminary'].min().sum()\n","# Groups the data by the values of the 'Original feature name' column.\n","# Aggregates the data in the 'Coefficients' column, calculating their minimum.\n","# Sums all minimum values.\n","min_sum_score_prel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v7Q0-aHEAB3g"},"outputs":[],"source":["max_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Preliminary'].max().sum()\n","# Groups the data by the values of the 'Original feature name' column.\n","# Aggregates the data in the 'Coefficients' column, calculating their maximum.\n","# Sums all maximum values.\n","max_sum_score_prel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KtJiGm9gAB3g"},"outputs":[],"source":["# One has to be subtracted from the maximum score for one original variable. Which one? We'll evaluate based on differences."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4A2PodlAAB3h"},"outputs":[],"source":["df_scorecard['Difference'] = df_scorecard['Score - Preliminary'] - df_scorecard['Score - Calculation']\n","df_scorecard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vcf6pDNbAB3h"},"outputs":[],"source":["df_scorecard['Score - Final'] = df_scorecard['Score - Preliminary']\n","df_scorecard['Score - Final'][77] = 16\n","df_scorecard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y5G_fSygAB3h"},"outputs":[],"source":["min_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Final'].min().sum()\n","# Groups the data by the values of the 'Original feature name' column.\n","# Aggregates the data in the 'Coefficients' column, calculating their minimum.\n","# Sums all minimum values.\n","min_sum_score_prel"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"t7E7FQgRAB3h"},"outputs":[],"source":["max_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Final'].max().sum()\n","# Groups the data by the values of the 'Original feature name' column.\n","# Aggregates the data in the 'Coefficients' column, calculating their maximum.\n","# Sums all maximum values.\n","max_sum_score_prel"]},{"cell_type":"markdown","metadata":{"id":"npXdXUsYAB3h"},"source":["### Caclulating Credit Score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"04bq-nREAB3h"},"outputs":[],"source":["inputs_test_with_ref_cat.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9HllMQ5EAB3h"},"outputs":[],"source":["df_scorecard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zzPGtLaAAB3h"},"outputs":[],"source":["inputs_test_with_ref_cat_w_intercept = inputs_test_with_ref_cat"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"Fo1s36C8AB3h"},"outputs":[],"source":["inputs_test_with_ref_cat_w_intercept.insert(0, 'Intercept', 1)\n","# We insert a column in the dataframe, with an index of 0, that is, in the beginning of the dataframe.\n","# The name of that column is 'Intercept', and its values are 1s."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GH9uQc1kAB3h"},"outputs":[],"source":["inputs_test_with_ref_cat_w_intercept.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I377LiDdAB3h"},"outputs":[],"source":["inputs_test_with_ref_cat_w_intercept = inputs_test_with_ref_cat_w_intercept[df_scorecard['Feature name'].values]\n","# Here, from the 'inputs_test_with_ref_cat_w_intercept' dataframe, we keep only the columns with column names,\n","# exactly equal to the row values of the 'Feature name' column from the 'df_scorecard' dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KlBYs_ZXAB3i"},"outputs":[],"source":["inputs_test_with_ref_cat_w_intercept.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VA_g8KnnAB3i"},"outputs":[],"source":["scorecard_scores = df_scorecard['Score - Final']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lu9MjRz8AB3i"},"outputs":[],"source":["inputs_test_with_ref_cat_w_intercept.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7uU-hmayAB3i"},"outputs":[],"source":["scorecard_scores.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KgWng1EbAB3i"},"outputs":[],"source":["scorecard_scores = scorecard_scores.values.reshape(102, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QJX1ECWEAB3i"},"outputs":[],"source":["scorecard_scores.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RvDxTajCAB3i"},"outputs":[],"source":["y_scores = inputs_test_with_ref_cat_w_intercept.dot(scorecard_scores)\n","# Here we multiply the values of each row of the dataframe by the values of each column of the variable,\n","# which is an argument of the 'dot' method, and sum them. It's essentially the sum of the products."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"joawzI24AB3i"},"outputs":[],"source":["y_scores.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KK356XZTAB3j"},"outputs":[],"source":["y_scores.tail()"]},{"cell_type":"markdown","metadata":{"id":"oSf9ikG7AB3j"},"source":["### From Credit Score to PD"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vjQFSE0xAB3j"},"outputs":[],"source":["sum_coef_from_score = ((y_scores - min_score) / (max_score - min_score)) * (max_sum_coef - min_sum_coef) + min_sum_coef\n","# We divide the difference between the scores and the minimum score by\n","# the difference between the maximum score and the minimum score.\n","# Then, we multiply that by the difference between the maximum sum of coefficients and the minimum sum of coefficients.\n","# Then, we add the minimum sum of coefficients."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jmjmcsU1AB3j"},"outputs":[],"source":["y_hat_proba_from_score = np.exp(sum_coef_from_score) / (np.exp(sum_coef_from_score) + 1)\n","# Here we divide an exponent raised to sum of coefficients from score by\n","# an exponent raised to sum of coefficients from score plus one.\n","y_hat_proba_from_score.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fNEcX38oAB3j"},"outputs":[],"source":["y_hat_test_proba[0: 5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JCHuTr-HAB3j"},"outputs":[],"source":["df_actual_predicted_probs['y_hat_test_proba'].head()"]},{"cell_type":"markdown","metadata":{"id":"HUB3AmZhAB3j"},"source":["### Setting Cut-offs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4nIh24deAB3j"},"outputs":[],"source":["# We need the confusion matrix again.\n","#np.where(np.squeeze(np.array(loan_data_targets_test)) == np.where(y_hat_test_proba >= tr, 1, 0), 1, 0).sum() / loan_data_targets_test.shape[0]\n","tr = 0.9\n","df_actual_predicted_probs['y_hat_test'] = np.where(df_actual_predicted_probs['y_hat_test_proba'] > tr, 1, 0)\n","#df_actual_predicted_probs['loan_data_targets_test'] == np.where(df_actual_predicted_probs['y_hat_test_proba'] >= tr, 1, 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"-Cw7e1egAB3k"},"outputs":[],"source":["pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJ1xcE28AB3k"},"outputs":[],"source":["pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wj8IbiuoAB3k"},"outputs":[],"source":["(pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[0, 0] + (pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[1, 1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K08vWrOQAB3k"},"outputs":[],"source":["from sklearn.metrics import roc_curve, roc_auc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I2q9ej0hAB3k"},"outputs":[],"source":["roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QWwKedW8AB3k"},"outputs":[],"source":["fpr, tpr, thresholds = roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h8v0QczqAB3l"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set()"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"rnuBIzVVAB3l"},"outputs":[],"source":["plt.plot(fpr, tpr)\n","plt.plot(fpr, fpr, linestyle = '--', color = 'k')\n","plt.xlabel('False positive rate')\n","plt.ylabel('True positive rate')\n","plt.title('ROC curve')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kqsFpMJMAB3l"},"outputs":[],"source":["thresholds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9_XaRUSOAB3l"},"outputs":[],"source":["thresholds.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GBmSGdkqAB3l"},"outputs":[],"source":["df_cutoffs = pd.concat([pd.DataFrame(thresholds), pd.DataFrame(fpr), pd.DataFrame(tpr)], axis = 1)\n","# We concatenate 3 dataframes along the columns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sdyvpm2sAB3l"},"outputs":[],"source":["df_cutoffs.columns = ['thresholds', 'fpr', 'tpr']\n","# We name the columns of the dataframe 'thresholds', 'fpr', and 'tpr'."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oQY52iXdAB3l"},"outputs":[],"source":["df_cutoffs.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qMTHEVYSAB3l"},"outputs":[],"source":["df_cutoffs['thresholds'][0] = 1 - 1 / np.power(10, 16)\n","# Let the first threshold (the value of the thresholds column with index 0) be equal to a number, very close to 1\n","# but smaller than 1, say 1 - 1 / 10 ^ 16."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KNG-st7VAB3l"},"outputs":[],"source":["df_cutoffs['Score'] = ((np.log(df_cutoffs['thresholds'] / (1 - df_cutoffs['thresholds'])) - min_sum_coef) * ((max_score - min_score) / (max_sum_coef - min_sum_coef)) + min_score).round()\n","# The score corresponsing to each threshold equals:\n","# The the difference between the natural logarithm of the ratio of the threshold and 1 minus the threshold and\n","# the minimum sum of coefficients\n","# multiplied by\n","# the sum of the minimum score and the ratio of the difference between the maximum score and minimum score and\n","# the difference between the maximum sum of coefficients and the minimum sum of coefficients."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XgpN4YC6AB3l"},"outputs":[],"source":["df_cutoffs.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T6_pgKwQAB3m"},"outputs":[],"source":["df_cutoffs['Score'][0] = max_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"smX-Hao6AB3m"},"outputs":[],"source":["df_cutoffs.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P--AnYYjAB3m"},"outputs":[],"source":["df_cutoffs.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n0ti1K9yAB3m"},"outputs":[],"source":["# We define a function called 'n_approved' which assigns a value of 1 if a predicted probability\n","# is greater than the parameter p, which is a threshold, and a value of 0, if it is not.\n","# Then it sums the column.\n","# Thus, if given any percentage values, the function will return\n","# the number of rows wih estimated probabilites greater than the threshold.\n","def n_approved(p):\n","    return np.where(df_actual_predicted_probs['y_hat_test_proba'] >= p, 1, 0).sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jdkbWD66AB3m"},"outputs":[],"source":["df_cutoffs['N Approved'] = df_cutoffs['thresholds'].apply(n_approved)\n","# Assuming that all credit applications above a given probability of being 'good' will be approved,\n","# when we apply the 'n_approved' function to a threshold, it will return the number of approved applications.\n","# Thus, here we calculate the number of approved appliations for al thresholds.\n","df_cutoffs['N Rejected'] = df_actual_predicted_probs['y_hat_test_proba'].shape[0] - df_cutoffs['N Approved']\n","# Then, we calculate the number of rejected applications for each threshold.\n","# It is the difference between the total number of applications and the approved applications for that threshold.\n","df_cutoffs['Approval Rate'] = df_cutoffs['N Approved'] / df_actual_predicted_probs['y_hat_test_proba'].shape[0]\n","# Approval rate equalts the ratio of the approved applications and all applications.\n","df_cutoffs['Rejection Rate'] = 1 - df_cutoffs['Approval Rate']\n","# Rejection rate equals one minus approval rate."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w4PHFAuCAB3m"},"outputs":[],"source":["df_cutoffs.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9vdfBZ7AB3m"},"outputs":[],"source":["df_cutoffs.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lHAWbGlCAB3m"},"outputs":[],"source":["df_cutoffs.iloc[5000: 6200, ]\n","# Here we display the dataframe with cutoffs form line with index 5000 to line with index 6200."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QSbOUj5-AB3m"},"outputs":[],"source":["df_cutoffs.iloc[1000: 2000, ]\n","# Here we display the dataframe with cutoffs form line with index 1000 to line with index 2000."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r5QC4hA_AB3m"},"outputs":[],"source":["inputs_train_with_ref_cat.to_csv('inputs_train_with_ref_cat.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rc6srtQxAB3n"},"outputs":[],"source":["df_scorecard.to_csv('df_scorecard.csv')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}